import json
from fastapi import FastAPI, Query, HTTPException # Import HTTPException
from fastapi.responses import JSONResponse, FileResponse # Import FileResponse for serving JSON file
from fastapi.middleware.cors import CORSMiddleware
import os
import duckdb
from deltalake import DeltaTable, __version__ as deltalake_version # Import DeltaTable and version
from typing import Optional, List # Import List for return type hint
import datetime
import math # For checking NaN
import numpy as np # Import numpy for replacing infinite values
import pandas as pd # Ensure pandas is imported

# --- Configuration ---\
BASE_DIR = '.'
LAKEHOUSE_PATH = os.path.join(BASE_DIR, 'lakehouse_data/lakehouse_disasters')
# --- Path to Farmer Registry Delta table ---
FARMER_REGISTRY_PATH = os.path.join(BASE_DIR, 'lakehouse_data/farmer_registry')
# --- Path to the pre-generated GeoJSON file ---
AGGREGATED_GEOJSON_FILE = os.path.join(BASE_DIR, 'api_output/api_data.json')
# Using in-memory DuckDB for raw queries

app = FastAPI()

# --- CORS Middleware ---\
origins = ["http://localhost", "http://localhost:8000", "http://127.0.0.1", "http://127.0.0.1:8000", "https://dawvinfosys.test", "https://27.110.161.135", "null"]
# origins = ["*"] # Allow all for easier local dev, restrict in production
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Helper Function to Clean Data for JSON ---
# (Handles NaN, infinity, dates etc.)
def clean_data_for_json(data):
    cleaned_data = []
    for row_dict in data:
        cleaned_row = {}
        for key, value in row_dict.items():
            if isinstance(value, (np.int64, np.int32)):
                cleaned_row[key] = int(value)
            elif isinstance(value, (np.float64, np.float32)):
                if np.isnan(value) or np.isinf(value):
                    cleaned_row[key] = None # Replace NaN/Inf with None
                else:
                    # Round floats that might represent percentages or specific values
                    if key == 'percentage_farmers_affected':
                         cleaned_row[key] = round(float(value), 2)
                    else:
                         cleaned_row[key] = float(value)
            elif isinstance(value, (datetime.date, datetime.datetime, pd.Timestamp)):
                 # Format dates as YYYY-MM-DD strings
                 cleaned_row[key] = value.strftime('%Y-%m-%d')
            elif pd.isna(value): # Catch pandas NA types (like NaT)
                 cleaned_row[key] = None
            else:
                cleaned_row[key] = value
        cleaned_data.append(cleaned_row)
    return cleaned_data


# --- Endpoint to serve the pre-aggregated GeoJSON map data ---
@app.get("/query", response_model=dict) # Response model ensures it returns a dictionary (GeoJSON)
async def get_aggregated_geojson_map_data():
    """
    Serves the pre-aggregated GeoJSON file generated by data_manager.py export.
    This endpoint returns the full aggregated dataset for the map.
    """
    print(f"API (/query): Attempting to serve aggregated GeoJSON from: {AGGREGATED_GEOJSON_FILE}")
    if not os.path.exists(AGGREGATED_GEOJSON_FILE):
        print(f"API ERROR (/query): Aggregated GeoJSON file not found at {AGGREGATED_GEOJSON_FILE}")
        raise HTTPException(status_code=404, detail=f"Aggregated data file not found. Please run 'python data_manager.py export'.")

    try:
        # Load into memory to return as JSON dictionary (FastAPI handles serialization)
        with open(AGGREGATED_GEOJSON_FILE, 'r') as f:
            geojson_data = json.load(f)
        print(f"API (/query): Successfully loaded and returning GeoJSON data.")
        return geojson_data

    except Exception as e:
        error_message = f"An error occurred reading the aggregated GeoJSON file: {str(e)}"
        print(f"API ERROR (/query): {error_message}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="Could not read aggregated map data.")


# --- Endpoint to serve raw/detailed data for charts and table ---
@app.get("/raw", response_model=dict) # Renamed from /api/disaster_summary
async def get_raw_disaster_data(
    province: Optional[str] = Query(None, description="Filter by province (case-insensitive, exact match)"),
    municipality: Optional[str] = Query(None, description="Filter by municipality (case-insensitive, exact match)"),
    disaster_category: Optional[str] = Query(None, description="Filter by disaster category (case-insensitive, exact match)"), # Kept category filter
    disaster_name: Optional[str] = Query(None, description="Filter by specific disaster name"), # Added disaster_name
    commodity: Optional[str] = Query(None, description="Filter by commodity name"), # Added commodity
    quarter: Optional[int] = Query(None, description="Filter by quarter (1-4)", ge=1, le=4),
    year: Optional[int] = Query(None, description="Filter by year (e.g., 2023)", ge=1900, le=2100),
    start_date: Optional[datetime.date] = Query(None, description="Filter by start date (YYYY-MM-DD) - inclusive"), # Changed type hint
    end_date: Optional[datetime.date] = Query(None, description="Filter by end date (YYYY-MM-DD) - inclusive"), # Changed type hint
    limit: int = Query(1000, description="Maximum number of records to return", gt=0, le=10000) # Increased limit slightly
):
    """
    Queries the raw disaster data from the Delta Lake table, optionally joins
    with farmer registry data, applies filters, and returns detailed records.
    """
    print(f"API (/raw): Received request with filters: start={start_date}, end={end_date}, prov={province}, mun={municipality}, cat={disaster_category}, name={disaster_name}, com={commodity}, year={year}, qtr={quarter}")
    con = None
    safe_lakehouse_path = os.path.normpath(LAKEHOUSE_PATH)
    safe_farmer_registry_path = os.path.normpath(FARMER_REGISTRY_PATH)
    farmer_registry_exists = os.path.exists(safe_farmer_registry_path)

    if not os.path.exists(safe_lakehouse_path):
        print(f"API ERROR (/raw): Main Delta table not found at '{safe_lakehouse_path}'.")
        raise HTTPException(status_code=404, detail=f"Main disaster data source not found. Run data import first.")
    if not farmer_registry_exists:
        print(f"API WARNING (/raw): Farmer registry table not found at '{safe_farmer_registry_path}'. Proceeding without join.")

    try:
        print("API (/raw): Connecting to DuckDB (in-memory)...")
        con = duckdb.connect(read_only=False)
        print(f"API (/raw): DuckDB Version: {duckdb.__version__}")
        print(f"API (/raw): Deltalake Library Version: {deltalake_version}") # Use imported version

        # --- Read Delta Table Directly (More Robust) ---
        print(f"API (/raw): Reading main Delta table from {safe_lakehouse_path}")
        # Use DuckDB's native Delta reading if available and working, otherwise fallback to pandas
        # This requires the DuckDB delta extension.
        try:
             con.sql("INSTALL delta; LOAD delta;")
             # Check for function existence might be needed depending on DuckDB version
             # For now, assume read_delta_table or similar exists if extension loads
             main_table_read_sql = f"SELECT * FROM read_delta('{safe_lakehouse_path}')"
             con.sql(f"CREATE OR REPLACE TEMPORARY VIEW main_disasters AS {main_table_read_sql};")
             print("API (/raw): Registered main_disasters view using DuckDB Delta extension.")
        except Exception as delta_ext_err:
             print(f"API WARNING (/raw): DuckDB Delta extension failed ({delta_ext_err}). Falling back to reading via pandas.")
             dt_main = DeltaTable(safe_lakehouse_path)
             df_main = dt_main.to_pandas()
             # Ensure date columns are datetime after pandas load
             if 'event_date_start' in df_main.columns: df_main['event_date_start'] = pd.to_datetime(df_main['event_date_start'], errors='coerce')
             if 'event_date_end' in df_main.columns: df_main['event_date_end'] = pd.to_datetime(df_main['event_date_end'], errors='coerce')
             con.register('main_disasters_df_pandas', df_main)
             con.sql("CREATE OR REPLACE TEMPORARY VIEW main_disasters AS SELECT * FROM main_disasters_df_pandas;")
             print(f"API (/raw): Registered main_disasters view using pandas fallback ({len(df_main)} rows).")


        # --- Pre-aggregate Farmer Data if exists ---
        farmer_join_view = None
        select_farmer_cols = ""
        select_calculated_cols = ""
        default_farmer_select = """,
            CAST(NULL AS INTEGER) AS registered_rice_farmers,
            CAST(NULL AS DOUBLE) AS total_declared_rice_area_ha,
            CAST(NULL AS DOUBLE) AS percentage_farmers_affected
        """ # Use CAST for explicit NULL types

        if farmer_registry_exists:
            print(f"API (/raw): Reading farmer registry Delta table from {safe_farmer_registry_path}")
            try:
                # Use DuckDB Delta extension if possible
                 farmer_table_read_sql = f"SELECT * FROM read_delta('{safe_farmer_registry_path}')"
                 con.sql(f"CREATE OR REPLACE TEMPORARY VIEW farmer_registry_raw AS {farmer_table_read_sql};")
                 print("API (/raw): Registered farmer_registry_raw view using DuckDB Delta extension.")
            except Exception as delta_ext_err_farmer:
                 print(f"API WARNING (/raw): DuckDB Delta extension failed for farmer data ({delta_ext_err_farmer}). Falling back via pandas.")
                 dt_farmer = DeltaTable(safe_farmer_registry_path)
                 df_farmer = dt_farmer.to_pandas()
                 con.register('farmer_registry_df_pandas', df_farmer)
                 con.sql("CREATE OR REPLACE TEMPORARY VIEW farmer_registry_raw AS SELECT * FROM farmer_registry_df_pandas;")
                 print(f"API (/raw): Registered farmer_registry_raw view using pandas fallback ({len(df_farmer)} rows).")

            # Aggregate farmer data
            con.sql("""
                CREATE OR REPLACE TEMPORARY VIEW farmer_summary AS
                SELECT
                    province,
                    municipality,
                    CAST(SUM(registered_rice_farmers) AS INTEGER) AS registered_rice_farmers,
                    SUM(total_declared_rice_area_ha) AS total_declared_rice_area_ha
                FROM farmer_registry_raw
                GROUP BY province, municipality;
            """)
            farmer_join_view = "farmer_summary"
            select_farmer_cols = """,
                fr.registered_rice_farmers,
                fr.total_declared_rice_area_ha
            """
            select_calculated_cols = """,
                CASE
                    WHEN fr.registered_rice_farmers > 0 THEN ROUND((CAST(d.farmers_affected AS DOUBLE) / fr.registered_rice_farmers) * 100, 2)
                    ELSE NULL
                END AS percentage_farmers_affected
            """
            default_farmer_select = "" # Clear default
            print("API (/raw): Created farmer_summary aggregation view.")


        # --- Dynamically Build WHERE Clause (Applies to disaster table 'd') ---
        where_clauses = []
        params = {}
        if province:
            where_clauses.append("UPPER(d.province) = UPPER($province)")
            params['province'] = province
        if municipality:
            where_clauses.append("UPPER(d.municipality) = UPPER($municipality)")
            params['municipality'] = municipality
        if disaster_category: # Keep category filter if needed
            where_clauses.append("UPPER(d.disaster_category) = UPPER($disaster_category)")
            params['disaster_category'] = disaster_category
        if disaster_name: # Filter by specific name
             where_clauses.append("UPPER(d.disaster_name) = UPPER($disaster_name)")
             params['disaster_name'] = disaster_name
        if commodity: # Filter by commodity
             where_clauses.append("UPPER(d.commodity) = UPPER($commodity)")
             params['commodity'] = commodity
        if start_date:
            # Use event_date_end >= start_date to catch events overlapping the start
            where_clauses.append("d.event_date_end >= $start_date")
            params['start_date'] = start_date
        if end_date:
            # Use event_date_start <= end_date to catch events overlapping the end
            where_clauses.append("d.event_date_start <= $end_date")
            params['end_date'] = end_date
        if quarter:
            # Ensure event_date_start is treated as DATE for quarter function
            where_clauses.append("quarter(CAST(d.event_date_start AS DATE)) = $quarter")
            params['quarter'] = quarter
        if year:
            # Ensure event_date_start is treated as DATE for year function
            # Or use the dedicated 'year' column if it exists and is reliable
            if 'year' in con.sql("DESCRIBE main_disasters;").df()['column_name'].values:
                 where_clauses.append("d.year = $year") # Use dedicated year column
            else:
                 where_clauses.append("year(CAST(d.event_date_start AS DATE)) = $year") # Fallback to extracting from date
            params['year'] = year

        where_sql = ""
        if where_clauses:
            where_sql = "WHERE " + " AND ".join(where_clauses)

        # --- Construct Final Query with Optional JOIN ---
        join_sql = ""
        if farmer_join_view:
            join_sql = f"""
            LEFT JOIN {farmer_join_view} AS fr
              ON UPPER(d.province) = UPPER(fr.province) AND UPPER(d.municipality) = UPPER(fr.municipality)
            """

        query = f"""
        SELECT
            d.* {select_farmer_cols} {select_calculated_cols} {default_farmer_select}
        FROM
            main_disasters AS d -- Use the view created earlier
        {join_sql} -- Add the join clause if applicable
        {where_sql} -- Apply filters using alias 'd'
        ORDER BY d.event_date_start DESC, d.province, d.municipality -- Use alias 'd'
        LIMIT $limit
        """
        params['limit'] = limit if limit > 0 else 1000 # Keep limit reasonable

        print(f"API (/raw): Executing query: {query}")
        print(f"API (/raw): With parameters: {params}")

        # Execute query with parameters
        result_df = con.execute(query, params).fetchdf()
        print(f"API (/raw): Query returned {len(result_df)} rows.")

        # Convert DataFrame to list of dictionaries and clean for JSON
        data = result_df.to_dict(orient='records')
        cleaned_data = clean_data_for_json(data)

        print("API (/raw): Returning cleaned data.")
        # Return data in the desired format {status, count, data}
        return {"status": "success", "count": len(cleaned_data), "data": cleaned_data}

    except HTTPException as http_exc:
        raise http_exc # Re-raise FastAPI specific exceptions
    except duckdb.Error as db_err: # Catch DuckDB specific errors
        error_message = f"Database query error: {str(db_err)}"
        print(f"API ERROR (/raw): {error_message}")
        import traceback; traceback.print_exc()
        raise HTTPException(status_code=500, detail="Error executing database query. Check server logs.")
    except Exception as e:
        error_message = f"An error occurred during raw data query execution: {str(e)}"
        print(f"API ERROR (/raw): {error_message}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="Could not retrieve raw data. Please check server logs or query parameters.")
    finally:
        if con:
            con.close()
            print("API (/raw): Closed DuckDB connection.")

# --- Root Endpoint ---
@app.get("/")
def root():
    # Simple message indicating the API is running
    return {"message": "DRRM Disaster Analysis API"}

